# import re
# import os 
# import time
# import itertools
# import datetime
# import random
# import numpy as np
# import pandas as pd
# from sklearn.feature_extraction.text import CountVectorizer
# from keras.models import Sequential
# from keras.layers import Dense, LSTM

# import database, l1_login


# personality_features_pre = np.array(pd.read_excel('/Users/takipon/Desktop/dprapp/sample.xlsx', index_col=0, header=0, sheet_name='personality_x2').columns)
# personality_features = np.delete(np.delete(personality_features_pre, 28), 27)





# #LSTM(RNN)
# class LSTM_RNN:

#     def make_sentense(act_txt_bbs):

#         #ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
#         os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'
#         start = time.time()

#         text_pre = pd.read_excel('/Users/takipon/Desktop/dprapp/sample.xlsx', index_col=None, header=0, sheet_name='sugesstion_list')
#         all_text = text_pre['text']
#         all_text = random.choice(all_text)

#         text = all_text + ' ' + act_txt_bbs + ' . '
#         print("æ–‡å­—æ•°",text)



#         #LSTMè¨­å®šã‚’ã™ã‚‹
#         n_rnn = 10 #æ™‚ç³»åˆ—ã®æ•°
#         batch_size = 128 
#         epochs = 200 #å¤šã„ã¨ç²¾åº¦ãŒä¸ŠãŒã‚‹ãŒã€ãƒ©ã‚°ã„ã®ã§20ã§
#         n_mid = 256 #ä¸­é–“å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°



#         #æ–‡å­—ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
#         # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨æ–‡å­—ã§è¾æ›¸ã‚’ä½œæˆ
#         chars = sorted(list(set(text)))  # setã§æ–‡å­—ã®é‡è¤‡ã‚’ãªãã—ã€å„æ–‡å­—ã‚’ãƒªã‚¹ãƒˆã«æ ¼ç´ã™ã‚‹
#         print("æ–‡å­—æ•°ï¼ˆé‡è¤‡ç„¡ã—ï¼‰", len(chars))
#         char_indices = {}  # æ–‡å­—ãŒã‚­ãƒ¼ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå€¤
#         for i, char in enumerate(chars):
#             char_indices[char] = i
#         indices_char = {}  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚­ãƒ¼ã§æ–‡å­—ãŒå€¤
#         for i, char in enumerate(chars):
#             indices_char[i] = char
        
#         # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¨ã€ãã‚Œã‹ã‚‰äºˆæ¸¬ã™ã¹ãæ–‡å­—ã‚’å–ã‚Šå‡ºã™
#         time_chars = []
#         next_chars = []
#         for i in range(0, len(text) - n_rnn):
#             time_chars.append(text[i: i + n_rnn])
#             next_chars.append(text[i + n_rnn])
        
#         # å…¥åŠ›ã¨æ­£è§£ã‚’one-hotè¡¨ç¾ã§è¡¨ã™ã€‚ï¼‘æ–‡å­—æ¯ã«0,1ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ•ãƒ«ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚
#         x = np.zeros((len(time_chars), n_rnn, len(chars)), dtype=np.bool)
#         t = np.zeros((len(time_chars), len(chars)), dtype=np.bool)
#         for i, t_cs in enumerate(time_chars):
#             t[i, char_indices[next_chars[i]]] = 1  # æ­£è§£ã‚’one-hotè¡¨ç¾ã§è¡¨ã™
#             for j, char in enumerate(t_cs):
#                 x[i, j, char_indices[char]] = 1  # å…¥åŠ›ã‚’one-hotè¡¨ç¾ã§è¡¨ã™

#         print("xã®å½¢çŠ¶", x.shape)
#         print("tã®å½¢çŠ¶", t.shape)



#         #LSTMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
#         model_lstm = Sequential()
#         model_lstm.add(LSTM(n_mid, input_shape=(n_rnn, len(chars))))
#         model_lstm.add(Dense(len(chars), activation="softmax"))
#         model_lstm.compile(loss='categorical_crossentropy', optimizer="adam")
#         print(model_lstm.summary())



#         #æ–‡ç« ç”Ÿæˆç”¨ã®é–¢æ•°
#         from keras.callbacks import LambdaCallback
        
#         def on_epoch_end(epoch, logs):
#             print("ã‚¨ãƒãƒƒã‚¯: ", epoch)

#             elapsed_time = time.time() - start
#             print ("on_epoch_end  elapsed_time:{0}".format(elapsed_time) + "[sec]")
            
#             beta = 4  # ç¢ºç‡åˆ†å¸ƒã‚’èª¿æ•´ã™ã‚‹å®šæ•°
#             prev_text = text[0:n_rnn]  # å…¥åŠ›ã«ä½¿ã†æ–‡å­—
#             created_text = prev_text  # ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            
#             print("ã‚·ãƒ¼ãƒ‰: ", created_text)

#             for i in range(100):
#                 # å…¥åŠ›ã‚’one-hotè¡¨ç¾ã«
#                 x_pred = np.zeros((1, n_rnn, len(chars)))
#                 for j, char in enumerate(prev_text):
#                     x_pred[0, j, char_indices[char]] = 1
                
#                 # äºˆæ¸¬ã‚’è¡Œã„ã€æ¬¡ã®æ–‡å­—ã‚’å¾—ã‚‹
#                 # yã®å½¢çŠ¶ã¯ã€1åˆ— 1049è¡Œ(æ–‡å­—æ•°=å‡ºåŠ›å±¤ã®æ•°)ã«ãªã£ã¦ã„ã‚‹
#                 y = model.predict(x_pred)
#                 #print(y.shape )
#                 p_power = y[0] ** beta  # ç¢ºç‡åˆ†å¸ƒã®èª¿æ•´(1049å€‹ã®é…åˆ—ã®ä¸­ã‹ã‚‰ã€ç¢ºç‡ãŒé«˜ã„æ–‡å­—ã‚’å–å¾—ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ã€€)
#                 next_index = np.random.choice(len(p_power), p=p_power/np.sum(p_power))        
#                 next_char = indices_char[next_index]

#                 created_text += next_char
#                 prev_text = prev_text[1:] + next_char

#             print(created_text)
#             print()
            


#         # ã‚¨ãƒãƒƒã‚¯çµ‚äº†å¾Œã«å®Ÿè¡Œã•ã‚Œã‚‹é–¢æ•°ã‚’è¨­å®š
#         epock_end_callback= LambdaCallback(on_epoch_end=on_epoch_end)



#         #å­¦ç¿’
#         model = model_lstm

#         elapsed_time = time.time() - start
#         print ("å­¦ç¿’é–‹å§‹ elapsed_time:{0}".format(elapsed_time) + "[sec]")
#         history_lstm = model_lstm.fit(x, t,
#                             batch_size=batch_size,
#                             epochs=epochs,
#                             callbacks=[epock_end_callback])





#         # def get_antifical_text(lstm_predict):
#         def get_antifical_text():
    
#             beta = 4  # ç¢ºç‡åˆ†å¸ƒã‚’èª¿æ•´ã™ã‚‹å®šæ•°
#             prev_text = text[0:n_rnn]  # å…¥åŠ›ã«ä½¿ã†æ–‡å­—
#             created_text = prev_text  # ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

#             for i in range(100):
#                 x_pred = np.zeros((1, n_rnn, len(chars)))
#                 for j, char in enumerate(prev_text):
#                     x_pred[0, j, char_indices[char]] = 1
                    
#                 y = model.predict(x_pred)
#                 p_power = y[0] ** beta  # ç¢ºç‡åˆ†å¸ƒã®èª¿æ•´(1049å€‹ã®é…åˆ—ã®ä¸­ã‹ã‚‰ã€ç¢ºç‡ãŒé«˜ã„æ–‡å­—ã‚’å–å¾—ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ã€€)
#                 next_index = np.random.choice(len(p_power), p=p_power/np.sum(p_power))        
#                 next_char = indices_char[next_index]

#                 created_text += next_char
#                 prev_text = prev_text[1:] + next_char

#             return created_text


#         # lstm_predict = model_lstm.predict(x, batch_size=batch_size)
#         # antifical_txt_pre = get_antifical_text(lstm_predict)
#         antifical_txt_pre = get_antifical_text()
#         # print('AIæ–‡ç« ',antifical_txt_pre)
#         antifical_txt = antifical_txt_pre.split()
#         # print(antifical_txt_pre)

#         ai_txt_num = antifical_txt.index('.')

#         #ã‚«ãƒ³ãƒãŒã‚ã‚‹ã¨ã“ã‚ã¾ã§ä½œã£ãŸæ–‡ç« ã‚’å˜èªã‚’å…¥ã‚Œã¦ã„ã
#         ai_txt = ''
#         for num in range(ai_txt_num):
#             ai_txt = ai_txt + ' ' + antifical_txt[num]

#         return ai_txt





# #å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° åŒã˜ç›®æ¨™
# def word_cut(x):

#     vect = CountVectorizer(stop_words='english').fit(x)
#     vect.transform(x)
#     words = vect.get_feature_names()
#     return words






# def suggest():
#     #å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
#     ip = l1_login.get_ip().pop()
#     personality_pre = np.array(database.l2_personality_last_record(ip))
#     personality_num = np.ravel(personality_pre)[2] #[1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1]
#     personality_num = personality_num.translate(str.maketrans({' ':'', '[':'', ']':''})) #101110001110101010010001101
#     print('1', personality_num)

#     all_personality_record_pre= np.array(database.l2_personality_all_record())
#     all_personality_record = pd.DataFrame(all_personality_record_pre).iloc[:, 2] #0~79 [1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0...
#     print('2', all_personality_record)



#     #å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° åŒã˜æ€§æ ¼
#     similarity_rate=[]
#     for num in range(len(all_personality_record)):

#         x_personality_num = all_personality_record[num].translate(str.maketrans({' ':'', '[':'', ']':''})) #101110001110101010010001101

#         common_features = []
#         for i in range(len(personality_features)):
#             if personality_num[i] == x_personality_num[i]:
#                 common_features.append(personality_features[i]) #ä¸¡è€…ã®å…±é€šã™ã‚‹æ€§æ ¼ã‚’æ¢ã™

#         if len(common_features) / len(personality_features) >= 0.7: #19/27=70%å…±é³´ #ğŸŒŸæœ¬ç•ªã§ã¯0.8ã«
#             similarity_rate.append(num) #æ€§æ ¼ãŒä¼¼ã¦ã‚‹äººãŸã¡ã®é…åˆ—ç•ªå·ãŒå…¥ã£ã¦ã‚‹
#     print('3', similarity_rate)



#     user_end_goal_pre = database.l2_endg_show(ip)
#     user_endg_task_pre = list(itertools.chain.from_iterable(user_end_goal_pre))[3].split(',') #[' build better relationships.', " work on goals you've given up on", 'Travel', '']
#     user_endg_task = word_cut(user_endg_task_pre) #['better', 'build', 'given', 'goals', 'relationships', 'travel', 've', 'work']
#     print('4', user_endg_task)



#     tips_list = []
#     end_goal_list = []
#     for num in similarity_rate:

#         other_user_ip = all_personality_record_pre[num][1] #æ€§æ ¼ä¼¼ã¦ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ipç²å¾—

#         other_users_end_goal_pre = database.l2_endg_show(other_user_ip)#ğŸŒŸæœ¬ç•ªã§ã¯ã“ã£ã¡
#         other_users_endg_pre = list(itertools.chain.from_iterable(other_users_end_goal_pre))[3].split(',') #ğŸŒŸæœ¬ç•ªã§ã¯ã“ã£ã¡#[' build better relationships.', " work on goals you've given up on", 'Travel', '']
#         other_user_endg_task = word_cut(other_users_endg_pre) #['better', 'build', 'given', 'goals', 'relationships', 'travel', 've', 'work'] #ğŸŒŸæœ¬ç•ªã§ã¯ã“ã£ã¡
#         # other_user_endg_task = ['better', 'build', 'BTS', 'sweden', 'relationships', 'programmer', 've', 'work','21'] 
#         print('5', other_user_endg_task)


#         tips = []
#         for x in user_endg_task:
#             for i in other_user_endg_task:
#                 if x == i:
#                     tips.append(1)
#                 else:
#                     tips.append(0)
#         tips_list.append(tips)

#     similarity_rate_endg = []
#     for num in range(len(tips_list)): #01ã‚»ãƒƒãƒˆãŸã¡ã®å€‹æ•°
#         if sum(tips_list[num]) / len(user_endg_task) >= 0.6: #ğŸŒŸæœ¬ç•ªã§ã¯0.8
#             similarity_rate_endg.append(similarity_rate[num]) #åŒã˜ç›®æ¨™ã‚’æŒã£ã¦ã‚‹äººãŸã¡ã®é…åˆ—ç•ªå·ãŒå…¥ã£ã¦ã‚‹
#     print('6', similarity_rate_endg)




#     #LDA 
#     other_user_txt_bbs = []
#     other_user_act_bbs = []
#     for num in similarity_rate_endg:
#         other_user_ip = all_personality_record_pre[num][1]
#         other_user_dairy = np.array(database.l2_dairy_show(other_user_ip))

#         mind = int(other_user_dairy[0][2]) #1ã¤ç›®ã‚’å–å¾—
#         print('7-1', other_user_dairy)
#         print('7-2', mind)

#         for next_mind in other_user_dairy: #2ã¤ç›®ã‹ã‚‰ã—ãŸã„ã‘ã©ã€ä½™è¨ˆãªã‚³ãƒ¼ãƒ‰ãŒå¢—ãˆã‚‹ã®ã§ã‚¹ãƒ«ãƒ¼
#             if int(next_mind[2]) > mind: #ğŸŒŸæœ¬ç•ªã¯>=ã§ã¯ãªã>ã®ã¿

#                 x_day = (next_mind[4] - mind_datetime).days #æ•°å€¤ä¸Šæ˜‡å‰æ—¥-æ•°å€¤ä¸Šæ˜‡æ—¥ã€‚ä½•æ—¥ã‹ã®ã¿å–å¾—ã€æ™‚é–“åˆ‡ã‚Šæ¨ã¦ 
#                 print('7-3', x_day)
#                 print('7-4', next_mind[4])
#                 print('7-5', mind_datetime)

#                 for day in range(-x_day):
#                     add_day = mind_datetime + datetime.timedelta(days = -day) #æ•°å€¤ä¸Šæ˜‡å‰æ—¥ã‹ã‚‰ï¼‘æ—¥ãšã¤è¿½åŠ  ğŸŒŸæœ¬ç•ªã§ã¯ã“ã£ã¡
#                     add_day = add_day.strftime('%Y-%m-%d') #æ™‚åˆ»ã‚’æ¨ã¦ã‚‹ ğŸŒŸæœ¬ç•ªã§ã¯ã“ã£ã¡
#                     # add_day = '2021-01-07'
#                     print('7-6', add_day)

#                     #æ²ç¤ºæ¿ã§ã®ç™ºè¨€å›å
#                     other_user_txt_bbs_pre = np.array(database.l3_bbs_txt_show_date(add_day))
#                     print('8', other_user_txt_bbs_pre)
#                     if len(other_user_txt_bbs_pre) != 0: #æ²ç¤ºæ¿ã§ã®ç™ºè¨€ãŒã‚ã£ãŸã‚‰
#                         for num in other_user_txt_bbs_pre:
#                             other_user_txt_bbs.append(num[2]) #æ²ç¤ºæ¿ã§ã®ç™ºè¨€ã‚’å›å

#                     #æ²ç¤ºæ¿ã§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å›å ã‚¢ã‚¯ã‚·ãƒ§ãƒ³â†’ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã—ãŸidâ†’æ²ç¤ºæ¿ã§ã®ç™ºè¨€id
#                     other_user_act_bbs_pre = np.array(database.l3_bbs_act_show_date(add_day))
#                     print('9', other_user_act_bbs_pre)

#                     if len(other_user_act_bbs_pre) > 0: #æ²ç¤ºæ¿ã§ã®è¡Œå‹•ãŒã‚ã£ãŸã‚‰
#                         print('9-1', other_user_act_bbs_pre)
#                         for num in other_user_act_bbs_pre:
#                             print('9-2', num)
#                             bbs_txt_pre = np.array(database.l3_bbs_txt_show_id(num[2]))
#                             print('9-3', bbs_txt_pre)
#                             bbs_txt = np.ravel(bbs_txt_pre)
#                             try:
#                                 other_user_act_bbs.append(bbs_txt[2]) #æ²ç¤ºæ¿ã§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å›å
#                             except IndexError:
#                                 pass
#                             print('10', other_user_act_bbs)

#             mind_datetime = next_mind[4]
#             mind = int(next_mind[2])

#         break #ğŸŒŸæœ¬ç•ªã§ã¯ã“ã‚Œã‚’è§£é™¤ å¤§é‡ã«ä¼¼ã¦ã‚‹äººãŒã„ã‚‹ã®ã§æ¼”ç®—ãŒé•·ããªã‚‹ã®ã§
    
#     if other_user_act_bbs == []:
#         ai_txt = 'sorry, we can not any suggestion'
#     else:
#         act_bbs = word_cut(other_user_act_bbs)
#         txt_bbs = word_cut(other_user_txt_bbs)
#         act_txt_bbs_pre = act_bbs + txt_bbs

#         act_txt_bbs = ''
#         for bbs in act_txt_bbs_pre:
#             act_txt_bbs = bbs +' '+ act_txt_bbs #today making great fuck day app today making great day app


#         ai_txt = LSTM_RNN.make_sentense(act_txt_bbs)
#         # print(ai_txt)

#     return ai_txt